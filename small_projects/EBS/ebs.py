# read file with velocities
# as you read each line, increment count for velocities that are within the last 6 months
# once you reach 200 (most recent ones) then just delete the rest
# take in argument (estimation for hours for that particular item)
# run 100 times, each time selecting a random velocity, output into hashmap the results that you get
# when selecting a random velocity, pick a number between 1 to n, and then average the two velocities in between (real number)
# get a probability distribution of possible actual times, output in a way that can be put into excel to make a graph
# maybe use some kind of python library, some kind of GUI to show a graph? might be hard
# overwrite the file with new data
